{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Halt_Dense.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOR1kWDOpDoqpdaWSNCosoA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10fb42df0b034a9ebae0264286a1b5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7cfb9c2728b940758406568aac532ee7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_af57fea9fc7841b9bf755990ddc6e237",
              "IPY_MODEL_329041fd062844e2bbd1cec4727ddc3c"
            ]
          }
        },
        "7cfb9c2728b940758406568aac532ee7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af57fea9fc7841b9bf755990ddc6e237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9e9753ac37874a68ad5346868e137afb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d4cb31c2300d45d69fa0882c2b29f20e"
          }
        },
        "329041fd062844e2bbd1cec4727ddc3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aa92b2de03dd40358b6c570e2ef2137c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:10&lt;00:00, 16072286.40it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11552807a29d4aa8995ccebeaef0739e"
          }
        },
        "9e9753ac37874a68ad5346868e137afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d4cb31c2300d45d69fa0882c2b29f20e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa92b2de03dd40358b6c570e2ef2137c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11552807a29d4aa8995ccebeaef0739e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdvOtjl90nFu"
      },
      "source": [
        "import torch #Pytorch used for Model creation\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms # 'datasets' gives us MNIST, 'transforms' allows use to turn images into tensors\n",
        "import matplotlib.pyplot as plt #To View Images\n",
        "import torch.nn as nn #To set up the model\n",
        "import torch.nn.functional as F #To calculate loss, use different activations etc.\n",
        "import torch.optim as optim #To access the adam optimizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import csv\n",
        "import torch.optim as optim\n",
        "from operator import add,truediv"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT5mLiDM034L"
      },
      "source": [
        "# Halt Dense Simulation\n",
        "during each model update <br>\n",
        "if accuracy > threshold:<br>\n",
        "> share convolutional and dense layers\n",
        "<br>else:<br>\n",
        "> share convolutional layers only\n",
        "\n",
        "## Classes needed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdrGCbww1V_N"
      },
      "source": [
        "# Super-class model\n",
        "'''\n",
        "https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
        "using VGG 3 which has 73% accuracy on Cifar-10\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(10, activation='softmax'))\n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.001, momentum=0.9)\n",
        "'''\n",
        "# Now use setup from FedMax\n",
        "Allen_Net = {'Conv': [32,'M',64,64,'M',64,64,'M'],\n",
        "             'MLP': [(1024,512),'ReLU',(512,10)]} # 'MLP' for user-readability, not used to generate dense layers\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self,net_setup):\n",
        "    super().__init__()\n",
        "    self.ConvLayers = self._make_conv(net_setup['Conv'])\n",
        "    self.MLP = nn.Sequential(nn.Linear(1024,512),nn.ReLU(True),nn.Linear(512,10))\n",
        "\n",
        "    # self.Conv1 = nn.Conv2d(3,32,(3,3))\n",
        "    # self.Conv2 = nn.Conv2d(32,32,(3,3))\n",
        "    # self.maxpool1 = nn.MaxPool2d((2,2))\n",
        "    # self.Conv3 = nn.Conv2d(32,64,(3,3))\n",
        "    # self.Conv4 = nn.Conv2d(64,64,(3,3))\n",
        "    # self.maxpool2 = nn.MaxPool2d((2,2))\n",
        "    # self.Conv5 = nn.Conv2d(64,128,(3,3))\n",
        "    # self.Conv6 = nn.Conv2d(128,128,(3,3))\n",
        "    # #self.maxpool3 = nn.MaxPool2d((2,2)) No \"same\" padding in torch, so size decreases\n",
        "    # self.flat = nn.Flatten()\n",
        "    # self.fc1 = nn.Linear(128,128)\n",
        "    # self.fc2 = nn.Linear(128,10)\n",
        "    #self.model = \n",
        "    # nn.Sequential(self.Conv1,nn.ReLU(),\n",
        "    #                            self.Conv2,nn.ReLU(),\n",
        "    #                            self.maxpool1,\n",
        "    #                            self.Conv3,nn.ReLU(),\n",
        "    #                            self.Conv4,nn.ReLU(),\n",
        "    #                            self.maxpool2,\n",
        "    #                            self.Conv5,nn.ReLU(),\n",
        "    #                            self.Conv6,nn.ReLU(),\n",
        "    #                            self.flat,\n",
        "    #                            self.fc1,nn.ReLU(),\n",
        "    #                            self.fc2,nn.Softmax())\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.optimizer = optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n",
        "  def _make_conv(self,layer_list):\n",
        "    layer = [] # add layers based on layer_list\n",
        "    in_channel = 3 # starting in_channel\n",
        "    for x in layer_list:\n",
        "      if x == 'M':# Add maxpool2d\n",
        "        layer += [nn.MaxPool2d(kernel_size = 2,stride = 2)]\n",
        "      else:\n",
        "        layer += [nn.Conv2d(in_channel,x,kernel_size = 3,padding=1),nn.BatchNorm2d(x),nn.ReLU(inplace=True)]\n",
        "        in_channel = x # set new channel size\n",
        "    return nn.Sequential(*layer) # make layers into sequential object\n",
        "  def forward(self,x):\n",
        "    out1 = self.ConvLayers(x) # conv\n",
        "    out2 = out1.view(out1.size(0),-1) # flatten\n",
        "    out3 = self.MLP(out2) # mlp\n",
        "    return F.log_softmax(out3,dim=1) # log softmax activation \n",
        "    #print(\"Adi\")\n",
        "    #return # self.model(x)\n",
        "  def getConvWeights(self):\n",
        "    return self.ConvLayers.state_dict()\n",
        "    # ConvList = []\n",
        "    # for layer in self.ConvLayers:\n",
        "    #   if isinstance(layer,nn.Conv2d):\n",
        "    #     ConvList.append(layer.state_dict())\n",
        "    #ConvList = [self.Conv1.weight,self.Conv2.weight,self.Conv3.weight,self.Conv4.weight,self.Conv5.weight,self.Conv6.weight]\n",
        "    #print(self.Conv1.weight.size())\n",
        "    #return ConvList\n",
        "  def getDenseWeights(self):\n",
        "    return self.ConvLayers.state_dict()\n",
        "    # DenseList = []\n",
        "    # for layer in self.MLP:\n",
        "    #   if isinstance(layer,nn.Linear):\n",
        "    #     DenseList.append(layer.state_dict())\n",
        "    #DenseList = [self.fc1.weight,self.fc2.weight]\n",
        "    #return DenseList\n",
        "  def setConvWeights(self,server):\n",
        "    self.ConvLayers.load_state_dict(server.ConvLayers.state_dict())\n",
        "    # self.Conv1.weight.new_tensor(server.Conv1.weight)\n",
        "    # self.Conv2.weight.new_tensor(server.Conv2.weight)\n",
        "    # self.Conv3.weight.new_tensor(server.Conv3.weight)\n",
        "    # self.Conv4.weight.new_tensor(server.Conv4.weight)\n",
        "    # self.Conv5.weight.new_tensor(server.Conv5.weight)\n",
        "    # self.Conv6.weight.new_tensor(server.Conv6.weight)\n",
        "  def setDenseWeights(self,server):\n",
        "    self.MLP.load_state_dict(server.MLP.state_dict())\n",
        "    # self.fc1.weight.new_tensor(server.fc1.weight)\n",
        "    # self.fc2.weight.new_tensor(server.fc2.weight) \n",
        "\n",
        "  # def set_weights(self,conv_list,dense_list): # won't use\n",
        "  #   self.Conv1.weight.new_tensor(conv_list[0])\n",
        "  #   self.Conv2.weight.new_tensor(conv_list[1])\n",
        "  #   self.Conv3.weight.new_tensor(conv_list[2])\n",
        "  #   self.Conv4.weight.new_tensor(conv_list[3])\n",
        "  #   self.Conv5.weight.new_tensor(conv_list[4])\n",
        "  #   self.Conv6.weight.new_tensor(conv_list[5])\n",
        "  #   self.fc1.weight.new_tensor(dense_list[0])\n",
        "  #   self.fc2.weight.new_tensor(dense_list[1])\n",
        "\n",
        "def train(model,dataloader,epochs):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  for epoch in range(epochs):\n",
        "    for i,data in enumerate(dataloader):\n",
        "      inputs,labels = data\n",
        "      model.optimizer.zero_grad()\n",
        "      outputs = model.forward(inputs)\n",
        "      loss = model.criterion(outputs,labels)\n",
        "      running_loss += float(loss)\n",
        "      loss.backward()\n",
        "      model.optimizer.step()\n",
        "  return running_loss\n",
        "    #get accuracy\n",
        "    # correct = 0\n",
        "    # total = 0\n",
        "    # with torch.no_grad():\n",
        "    #   for data in dataloader:\n",
        "    #     images,labels = data\n",
        "    #     outputs = self.forward(images)\n",
        "    #     _,prediction = torch.max(outputs,1) # choose classification based on highest probability\n",
        "    #     total+= labels.size(0)\n",
        "    #     correct += (labels == prediction).sum().item()\n",
        "    # return float (correct/total)\n",
        "\n",
        "\n",
        "#Testing functions\n",
        "def test(model,testloader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data,target in testloader:\n",
        "      #data,target = data.cuda(),target.cuda()\n",
        "      output = model(data)\n",
        "      test_loss += F.nll_loss(output,target,reduction='sum').item()\n",
        "      pred = output.argmax(dim=1,keepdim=True)\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "  test_loss /= len(testloader.dataset)\n",
        "  acc = correct/len(testloader.dataset)\n",
        "  return test_loss,acc"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du25BRnMATlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06857bb4-1dc1-4dfe-b942-19252b5318be"
      },
      "source": [
        "batch = 23\n",
        "color_channels = 3\n",
        "img_size = 32\n",
        "x = torch.randn(batch,color_channels,img_size,img_size)\n",
        "server = Model(Allen_Net)\n",
        "#server(x) # it works! May 29th 4:54pm\n",
        "x = server.getConvWeights()\n",
        "#x[0].size()\n",
        "y = server.getDenseWeights()\n",
        "#y[0].size() # should be 128x128\n",
        "\n",
        "device1 = Model(Allen_Net) # test of setConvWeights\n",
        "x = server.getConvWeights()\n",
        "print(id(x))\n",
        "device1.setConvWeights(server)\n",
        "y = device1.getConvWeights()\n",
        "print(id(y))\n",
        "for i,j in zip(x,y):\n",
        "  assert id(x) != id(y) # shouldn't be at the same address \n",
        "\n",
        "device1 = Model(Allen_Net) # test of setConvWeights\n",
        "x = server.getDenseWeights()\n",
        "print(id(x))\n",
        "device1.setDenseWeights(server)\n",
        "y = device1.getDenseWeights()\n",
        "print(id(y))\n",
        "for i,j in zip(x,y):\n",
        "  assert id(x) != id(y) # shouldn't be at the same address \n",
        "\n",
        "print(\"Passed prelim tests\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "139650388845920\n",
            "139650388576592\n",
            "139650388575872\n",
            "139650388575440\n",
            "Passed prelim tests\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvHhpZw4rDC8"
      },
      "source": [
        "### Load Cifar-10 and practice training\n",
        "\n",
        "Cifar loading code borrowed from:<br>\n",
        "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSPJKV8oBO2w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "10fb42df0b034a9ebae0264286a1b5f2",
            "7cfb9c2728b940758406568aac532ee7",
            "af57fea9fc7841b9bf755990ddc6e237",
            "329041fd062844e2bbd1cec4727ddc3c",
            "9e9753ac37874a68ad5346868e137afb",
            "d4cb31c2300d45d69fa0882c2b29f20e",
            "aa92b2de03dd40358b6c570e2ef2137c",
            "11552807a29d4aa8995ccebeaef0739e"
          ]
        },
        "outputId": "188ba5e5-9909-47a2-b776-baa4574d231a"
      },
      "source": [
        "print(\"Load Cifar and practice training\")\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "num_clients = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, # for server\n",
        "                                          shuffle=True, num_workers=2)\n",
        "#from : https://towardsdatascience.com/preserving-data-privacy-in-deep-learning-part-1-a04894f78029\n",
        "# Dividing the training data into num_clients, with each client having equal number of images\n",
        "traindata_split = torch.utils.data.random_split(trainset, [int(trainset.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
        "#^ for clients\n",
        "\n",
        "# Creating a pytorch loader for a Deep Learning model\n",
        "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load Cifar and practice training\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10fb42df0b034a9ebae0264286a1b5f2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1hyysLNtGfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b685c19b-ebe8-44a9-9ffb-98c150393d75"
      },
      "source": [
        "print(\"Set up devices for vanilla FL\")\n",
        "results_dict = dict()\n",
        "num_devices = num_clients\n",
        "for i in range(num_devices):\n",
        "  results_dict[\"Device \"+str(i)] = []\n",
        "results_dict[\"Server\"] = []\n",
        "communication_rounds = 10\n",
        "epoches = 1\n",
        "devices = []\n",
        "for i in range(num_devices):\n",
        "  devices.append(Model(Allen_Net))\n",
        "server = Model(Allen_Net)\n",
        "\n",
        "for i in range(communication_rounds):\n",
        "  print(\"Starting Communication Round %d\"%(i))\n",
        "  print(\"Transfer Global Model to devices\")\n",
        "  for device in devices:\n",
        "    device.setConvWeights(server)\n",
        "    device.setDenseWeights(server)\n",
        "\n",
        "  print(\"Training each device\")\n",
        "  for i,device in enumerate(devices):\n",
        "    loss = train(device,train_loader[i],epoches)\n",
        "    test_l,acc = test(device,train_loader[i])\n",
        "    results_dict[\"Device \"+str(i)].append([loss,test_l,acc])\n",
        "    print(\"Device %d has training loss %f, testing loss %f, acc %f\"%(i,loss,test_l,acc))\n",
        "  \n",
        "  print(\"Aggregate device models to server\")\n",
        "\n",
        "  global_dict = server.state_dict()\n",
        "  for k in global_dict.keys():\n",
        "    global_dict[k] = torch.stack([devices[i].state_dict()[k].float() for i in range(num_devices)],0).mean(0)\n",
        "  server.load_state_dict(global_dict)\n",
        "  # global_conv = []\n",
        "  # global_dense = []\n",
        "  # for device in devices:\n",
        "  #   global_conv.append(device.getConvWeights())\n",
        "  #   global_dense.append(device.getDenseWeights())\n",
        "  # x = global_conv[0]\n",
        "  # y = global_dense[0]\n",
        "  # for idx in range(1,num_devices,1):#from index 1 to the end\n",
        "  #   x = list(map(add,x,global_conv[idx]))\n",
        "  #   y = list(map(add,y,global_dense[idx]))\n",
        "  # x = [torch.div(conv,num_devices) for conv in x]\n",
        "  # y = [torch.div(dense,num_devices) for dense in y]\n",
        "  # server.set_weights(x,y)\n",
        "  #train_loss = server.train(trainloader,epoches)\n",
        "  test_loss,acc = test(server,testloader)\n",
        "  print(\"Server Accuracy %f, test loss %f\"%(acc,test_loss))\n",
        "  results_dict[\"Server\"].append([test_loss,acc])\n",
        "print(\"Results Dict\")\n",
        "print(results_dict)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Set up devices for vanilla FL\n",
            "Starting Communication Round 0\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has training loss 5082.947756, testing loss 1.234364, acc 0.564080\n",
            "Device 1 has training loss 5115.703791, testing loss 1.282281, acc 0.535280\n",
            "Device 2 has training loss 5165.173704, testing loss 1.310259, acc 0.522480\n",
            "Device 3 has training loss 5039.863138, testing loss 1.256583, acc 0.537840\n",
            "Aggregate device models to server\n",
            "Server Accuracy 0.360300, test loss 1.791189\n",
            "Starting Communication Round 1\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has training loss 4161.594296, testing loss 1.070539, acc 0.623600\n",
            "Device 1 has training loss 4148.629567, testing loss 1.081493, acc 0.614000\n",
            "Device 2 has training loss 4174.439868, testing loss 1.127822, acc 0.602160\n",
            "Device 3 has training loss 4108.355708, testing loss 1.034918, acc 0.630560\n",
            "Aggregate device models to server\n",
            "Server Accuracy 0.633100, test loss 1.036473\n",
            "Starting Communication Round 2\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has training loss 3489.746498, testing loss 0.896341, acc 0.681520\n",
            "Device 1 has training loss 3514.863392, testing loss 0.886083, acc 0.677760\n",
            "Device 2 has training loss 3490.709682, testing loss 0.955466, acc 0.649760\n",
            "Device 3 has training loss 3455.437468, testing loss 0.907654, acc 0.674400\n",
            "Aggregate device models to server\n",
            "Server Accuracy 0.696700, test loss 0.872306\n",
            "Starting Communication Round 3\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has training loss 3085.337010, testing loss 0.787205, acc 0.725520\n",
            "Device 1 has training loss 3087.921257, testing loss 0.892592, acc 0.679520\n",
            "Device 2 has training loss 3054.677117, testing loss 0.829002, acc 0.700480\n",
            "Device 3 has training loss 3008.149045, testing loss 0.765781, acc 0.724400\n",
            "Aggregate device models to server\n",
            "Server Accuracy 0.724700, test loss 0.800987\n",
            "Starting Communication Round 4\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has training loss 2787.334207, testing loss 0.749470, acc 0.734560\n",
            "Device 1 has training loss 2816.121491, testing loss 0.643018, acc 0.775440\n",
            "Device 2 has training loss 2746.876586, testing loss 0.667296, acc 0.768000\n",
            "Device 3 has training loss 2722.120567, testing loss 0.688615, acc 0.755920\n",
            "Aggregate device models to server\n",
            "Server Accuracy 0.748000, test loss 0.740582\n",
            "Starting Communication Round 5\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has training loss 2572.874337, testing loss 0.608606, acc 0.791040\n",
            "Device 1 has training loss 2581.469010, testing loss 0.653557, acc 0.772560\n",
            "Device 2 has training loss 2527.999022, testing loss 0.653943, acc 0.766000\n",
            "Device 3 has training loss 2481.719617, testing loss 0.607482, acc 0.787440\n",
            "Aggregate device models to server\n",
            "Server Accuracy 0.763500, test loss 0.695322\n",
            "Starting Communication Round 6\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has training loss 2373.810217, testing loss 0.645350, acc 0.765680\n",
            "Device 1 has training loss 2380.239057, testing loss 0.606051, acc 0.786160\n",
            "Device 2 has training loss 2348.297858, testing loss 0.549848, acc 0.810800\n",
            "Device 3 has training loss 2321.227133, testing loss 0.577311, acc 0.795840\n",
            "Aggregate device models to server\n",
            "Server Accuracy 0.777600, test loss 0.661125\n",
            "Starting Communication Round 7\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has training loss 2205.131358, testing loss 0.513890, acc 0.820400\n",
            "Device 1 has training loss 2228.420393, testing loss 0.599499, acc 0.787280\n",
            "Device 2 has training loss 2160.551740, testing loss 0.496783, acc 0.828160\n",
            "Device 3 has training loss 2154.353114, testing loss 0.514412, acc 0.817520\n",
            "Aggregate device models to server\n",
            "Server Accuracy 0.785800, test loss 0.633711\n",
            "Starting Communication Round 8\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has training loss 2044.601779, testing loss 0.487530, acc 0.828640\n",
            "Device 1 has training loss 2071.743410, testing loss 0.521264, acc 0.816880\n",
            "Device 2 has training loss 2014.250090, testing loss 0.532038, acc 0.805440\n",
            "Device 3 has training loss 2001.315280, testing loss 0.504250, acc 0.822320\n",
            "Aggregate device models to server\n",
            "Server Accuracy 0.783400, test loss 0.641837\n",
            "Starting Communication Round 9\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has training loss 1931.929573, testing loss 0.462939, acc 0.843040\n",
            "Device 1 has training loss 1933.623250, testing loss 0.469388, acc 0.836640\n",
            "Device 2 has training loss 1862.431344, testing loss 0.429920, acc 0.855280\n",
            "Device 3 has training loss 1876.416063, testing loss 0.500118, acc 0.822560\n",
            "Aggregate device models to server\n",
            "Server Accuracy 0.799700, test loss 0.595513\n",
            "Results Dict\n",
            "{'Device 0': [[5082.947756230831, 1.2343644268274307, 0.56408], [4161.594296067953, 1.0705388158738613, 0.6236], [3489.746497567743, 0.8963413071450591, 0.68152], [3085.3370104609057, 0.7872052260463684, 0.72552], [2787.3342068810016, 0.7494704707218707, 0.73456], [2572.8743369691074, 0.6086056890462339, 0.79104], [2373.8102165148593, 0.6453500347013772, 0.76568], [2205.131357696373, 0.5138896793190018, 0.8204], [2044.60177878564, 0.4875303033100255, 0.82864], [1931.9295727766585, 0.4629390082930773, 0.84304]], 'Device 1': [[5115.703791290522, 1.28228107029438, 0.53528], [4148.629567280412, 1.0814933276629448, 0.614], [3514.8633920922875, 0.886082500725016, 0.67776], [3087.9212574735284, 0.8925922432255745, 0.67952], [2816.121490964666, 0.6430175423125922, 0.77544], [2581.4690102536697, 0.6535565567675233, 0.77256], [2380.239056623075, 0.60605069955796, 0.78616], [2228.420392827131, 0.599498865116015, 0.78728], [2071.7434103282867, 0.5212636278972961, 0.81688], [1933.6232499754988, 0.4693878666247055, 0.83664]], 'Device 2': [[5165.1737041175365, 1.3102587216162682, 0.52248], [4174.439868122339, 1.1278216897398234, 0.60216], [3490.7096824757755, 0.9554657885736227, 0.64976], [3054.67711701151, 0.8290015700608492, 0.70048], [2746.8765858225524, 0.6672959705349057, 0.768], [2527.999022326432, 0.6539430109286308, 0.766], [2348.2978575946763, 0.5498482582569867, 0.8108], [2160.551740444731, 0.49678255160775037, 0.82816], [2014.2500904584303, 0.5320376902475581, 0.80544], [1862.431344291428, 0.42992032662821, 0.85528]], 'Device 3': [[5039.863137573004, 1.256583000459671, 0.53784], [4108.35570833087, 1.034917921332121, 0.63056], [3455.4374678581953, 0.9076540154385566, 0.6744], [3008.1490453369915, 0.7657807112495602, 0.7244], [2722.1205673310906, 0.6886154072493315, 0.75592], [2481.7196168499067, 0.6074820920655877, 0.78744], [2321.227132882457, 0.577310964832902, 0.79584], [2154.3531140529085, 0.5144118494770303, 0.81752], [2001.3152797990479, 0.5042497869655304, 0.82232], [1876.4160629058024, 0.5001182392876782, 0.82256]], 'Server': [[1.791189040851593, 0.3603], [1.0364729088753462, 0.6331], [0.8723062273774296, 0.6967], [0.8009874275865033, 0.7247], [0.7405818013698329, 0.748], [0.695321643771138, 0.7635], [0.6611248622483574, 0.7776], [0.6337108630706091, 0.7858], [0.6418373847729526, 0.7834], [0.5955128319623647, 0.7997]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g91T3zcDer-",
        "outputId": "8ead7aad-93b8-44da-a904-2b167d6801e3"
      },
      "source": [
        "# to continue training: old needs to be changined for iid data: june 11th\n",
        "for i in range(3,10):\n",
        "  print(\"Starting Communication Round %d\"%(i))\n",
        "  print(\"Transfer Global Model to devices\")\n",
        "  for device in devices:\n",
        "    device.setConvWeights(server)\n",
        "    device.setDenseWeights(server)\n",
        "\n",
        "  print(\"Training each device\")\n",
        "  for i,device in enumerate(devices):\n",
        "    accuracy = device.train(trainloader,epoches)\n",
        "    results_dict[\"Device \"+str(i)].append(accuracy)\n",
        "    print(\"Device %d has accuracy %f\"%(i,accuracy))\n",
        "  \n",
        "  print(\"Aggregate device models to server\")\n",
        "  global_conv = []\n",
        "  global_dense = []\n",
        "  for device in devices:\n",
        "    global_conv.append(device.getConvWeights())\n",
        "    global_dense.append(device.getDenseWeights())\n",
        "  x = global_conv[0]\n",
        "  y = global_dense[0]\n",
        "  for idx in range(1,num_devices,1):#from index 1 to the end\n",
        "    x = list(map(add,x,global_conv[idx]))\n",
        "    y = list(map(add,y,global_dense[idx]))\n",
        "  x = [torch.div(conv,num_devices) for conv in x]\n",
        "  y = [torch.div(dense,num_devices) for dense in y]\n",
        "  server.set_weights(x,y)\n",
        "  accuracy = server.train(trainloader,epoches)\n",
        "  results_dict[\"Server\"].append(accuracy)\n",
        "print(\"Results Dict\")\n",
        "print(results_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Communication Round 3\n",
            "Transfer Global Model to devices\n",
            "Training each device\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Device 0 has accuracy 0.101000\n",
            "Device 1 has accuracy 0.100000\n",
            "Device 2 has accuracy 0.100000\n",
            "Device 3 has accuracy 0.100000\n",
            "Aggregate device models to server\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting Communication Round 4\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has accuracy 0.285720\n",
            "Device 1 has accuracy 0.100000\n",
            "Device 2 has accuracy 0.100000\n",
            "Device 3 has accuracy 0.170760\n",
            "Aggregate device models to server\n",
            "Starting Communication Round 5\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has accuracy 0.340680\n",
            "Device 1 has accuracy 0.232540\n",
            "Device 2 has accuracy 0.100060\n",
            "Device 3 has accuracy 0.321460\n",
            "Aggregate device models to server\n",
            "Starting Communication Round 6\n",
            "Transfer Global Model to devices\n",
            "Training each device\n",
            "Device 0 has accuracy 0.374020\n",
            "Device 1 has accuracy 0.306640\n",
            "Device 2 has accuracy 0.234840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C9dBBPerTtd"
      },
      "source": [
        "print(\"Results Dict\")\n",
        "print(results_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApDadrQt1_gd"
      },
      "source": [
        "# check aggregation\n",
        "global_conv = []\n",
        "global_dense = []\n",
        "for device in devices:\n",
        "  global_conv.append(device.getConvWeights())\n",
        "  global_dense.append(device.getDenseWeights())\n",
        "#print(global_conv)\n",
        "x = global_conv[0]\n",
        "y = global_dense[0]\n",
        "#print(x[0][0])\n",
        "for idx in range(1,num_devices,1):\n",
        "  print(idx)\n",
        "  x = list(map(add,x,global_conv[idx]))\n",
        "  y = list(map(add,y,global_dense[idx]))\n",
        "print(x[0][0])\n",
        "adi = input(\"continue\")\n",
        "x = [torch.div(conv,num_devices) for conv in x]\n",
        "y = [torch.div(dense,num_devices) for dense in y]\n",
        "print(x[0][0])\n",
        "# it works"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmH6L60d3GCB"
      },
      "source": [
        "print(global_conv[0][0][0])\n",
        "print(global_conv[1][0][0])\n",
        "print(global_conv[2][0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4UV8Oq-4yFX"
      },
      "source": [
        "len(global_conv)\n",
        "print(range(2,num_devices,1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlJ0x9JIzJRv"
      },
      "source": [
        "server.train(trainloader,epoches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f6Thc7ezDOn"
      },
      "source": [
        "a = torch.randn(4, 4)\n",
        "b = torch.randn(4, 4)\n",
        "torch.mean(Tensor(a,b))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}